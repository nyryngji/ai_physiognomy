{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib.request\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 웹에서 이미지 크롤링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scroll_down(driver):\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # 페이지 맨 아래로 스크롤\n",
    "#         driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "#         time.sleep(2)\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             try:\n",
    "#                 load_more_button = driver.find_element(By.XPATH, '//input[@value=\"더보기\"]')\n",
    "#                 if load_more_button.is_displayed():\n",
    "#                     load_more_button.click()\n",
    "#                     time.sleep(2)\n",
    "#             except:\n",
    "#                 break\n",
    "#         last_height = new_height\n",
    "\n",
    "# query = input(\"검색어: \")\n",
    "# image_cnt = int(input(\"수집할 이미지 개수: \"))\n",
    "\n",
    "# url = 'https://search.naver.com/search.naver?ssc=tab.image.all&where=image&sm=tab_jum&query=%EB%B9%84%EC%88%91+%ED%94%84%EB%A6%AC%EC%A0%9C+%ED%86%A0%EB%A6%AC'\n",
    "\n",
    "\n",
    "# os.chdir('D:\\\\physiognomy\\\\dsf')\n",
    "\n",
    "# driver = webdriver.Chrome()  # Chrome 웹 드라이버 실행\n",
    "# driver.get(url)  # 검색어를 포함한 URL로 이동\n",
    "\n",
    "# scroll_down(driver)  # 페이지 스크롤 함수 호출\n",
    "# time.sleep(2)\n",
    "\n",
    "# # 이미지 정보 추출\n",
    "# soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "# image_info_list = soup.find_all('img', class_='_fe_image_tab_content_thumbnail_image')\n",
    "# image_and_name_list = []\n",
    "\n",
    "# print('=== 이미지 수집 시작 ===')\n",
    "\n",
    "# download_cnt = 0\n",
    "# for i in range(len(image_info_list)):\n",
    "#     if download_cnt == image_cnt:\n",
    "#         break\n",
    "#     try:\n",
    "#         if 'data-src' in image_info_list[i].attrs:\n",
    "#             save_image = image_info_list[i]['data-src']\n",
    "#         elif 'src' in image_info_list[i].attrs:\n",
    "#             save_image = image_info_list[i]['src']\n",
    "#         else:\n",
    "#             continue\n",
    "#         image_path = os.path.join(query.replace(' ', '_') + '_' + str(download_cnt+330) + '.jpg')\n",
    "#         print(image_path)\n",
    "#         image_and_name_list.append((save_image, image_path))\n",
    "#         download_cnt += 1\n",
    "#     except Exception as e:\n",
    "#         print(f\"이미지 URL 추출 실패: {e}\")\n",
    "\n",
    "# # 이미지 다운로드\n",
    "# for i in range(len(image_and_name_list)):\n",
    "#     try:\n",
    "#         urllib.request.urlretrieve(image_and_name_list[i][0], image_and_name_list[i][1])\n",
    "#         print(f\"다운로드 완료: {image_and_name_list[i][1]}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"이미지 다운로드 실패: {e}\")\n",
    "\n",
    "# print('=== 이미지 수집 종료 ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터 전처리\n",
    "\n",
    "- 위의 크롤링한 사진 중에 강아지만 인식해서 파일을 가져옴\n",
    "- 불필요한 특성 추출 방지를 위해서 객체 인식 방식을 선택함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOLOv5 모델 로드\n",
    "# os.chdir('D:\\\\physiognomy')\n",
    "\n",
    "# model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# for i in os.listdir('D:\\\\physiognomy\\\\dsf'):\n",
    "#     img_path = 'D:\\\\physiognomy\\\\dsf\\\\' + i # 여기에 이미지 경로를 입력하세요.\n",
    "#     img = cv2.imread(img_path)\n",
    "\n",
    "#     # 이미지를 모델에 입력\n",
    "#     results = model(img)\n",
    "\n",
    "#     # 결과를 DataFrame으로 변환\n",
    "#     df = results.pandas().xyxy[0]\n",
    "\n",
    "#     # 'name' 열에서 'dog' 라벨 필터링\n",
    "#     dog_df = df[df['name'] == 'dog']\n",
    "\n",
    "#     # 개 인식 결과 출력\n",
    "#     print(dog_df)\n",
    "\n",
    "#     # 개가 인식되지 않은 경우 처리\n",
    "#     if dog_df.empty:\n",
    "#         print(\"No dogs detected in the image.\")\n",
    "#     else:\n",
    "#         # 여러 개의 개가 있을 경우 첫 번째 개를 선택 (여러 개를 모두 자르고 싶다면 루프 사용)\n",
    "#         first_dog = dog_df.iloc[0]\n",
    "\n",
    "#         # 경계 상자의 좌표\n",
    "#         xmin, ymin, xmax, ymax = int(first_dog['xmin']), int(first_dog['ymin']), int(first_dog['xmax']), int(first_dog['ymax'])\n",
    "\n",
    "#         # 이미지 자르기\n",
    "#         cropped_img = img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "#         # 자른 이미지 표시\n",
    "#         cv2.imshow('dog',cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "#         cv2.waitKey(5)\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "#         # 자른 이미지 저장 (선택 사항)\n",
    "#         cv2.imwrite('D:\\\\physiognomy\\\\dog_face\\\\'+i, cropped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(path):\n",
    "    os.chdir(path)\n",
    "    files = os.listdir(path)\n",
    "    images = [cv2.imread(os.path.join(path, i),cv2.IMREAD_GRAYSCALE) for i in files]\n",
    "    resized_images = [cv2.resize(img, (120,120), interpolation=cv2.INTER_LINEAR) for img in images]\n",
    "    x = np.array(resized_images)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\physiognomy\\\\dog_face'\n",
    "file_names = os.listdir(file_path)\n",
    "\n",
    "y = []\n",
    "\n",
    "for i in file_names:\n",
    "    if 'bi' in i:\n",
    "        y += [1]\n",
    "    elif 'pome' in i:\n",
    "        y += [2]\n",
    "    elif 'pug' in i:\n",
    "        y += [3]\n",
    "    elif 'siberian_' in i:\n",
    "        y += [4]\n",
    "    elif 'chiwawa_' in i:\n",
    "        y += [5]\n",
    "    elif 'golden_' in i:\n",
    "        y += [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data(\"D:\\\\physiognomy\\\\dog_face\")\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6]), array([210, 194, 266, 202, 229, 232], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,random_state=100, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3, 4, 5, 6]), array([168, 153, 213, 166, 177, 189], dtype=int64))\n",
      "(array([1, 2, 3, 4, 5, 6]), array([42, 41, 53, 36, 52, 43], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(ytrain,return_counts=True))\n",
    "print(np.unique(ytest,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 120, 120, 120)     1200      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 60, 60, 120)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 240)       259440    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 60, 240)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 240)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 216000)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               25920120  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                7260      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 427       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26188447 (99.90 MB)\n",
      "Trainable params: 26188447 (99.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(120,kernel_size=3,activation='relu',padding='same',input_shape=(120,120,1)))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Conv2D(240,kernel_size=(3,3),activation='relu',padding='same'))\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(120, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "model.add(keras.layers.Dense(60, activation='relu'))\n",
    "model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\physiognomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정규화\n",
    "xtrain_scaled = xtrain/255\n",
    "xtest_scaled = xtest/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "34/34 [==============================] - ETA: 0s - loss: 2.0851 - accuracy: 0.2092"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 198s 6s/step - loss: 2.0851 - accuracy: 0.2092\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5311 - accuracy: 0.4090WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 192s 6s/step - loss: 1.5311 - accuracy: 0.4090\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3257 - accuracy: 0.4747WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 194s 6s/step - loss: 1.3257 - accuracy: 0.4747\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1745 - accuracy: 0.5366WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 199s 6s/step - loss: 1.1745 - accuracy: 0.5366\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0487 - accuracy: 0.5966WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 181s 5s/step - loss: 1.0487 - accuracy: 0.5966\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.6463WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 180s 5s/step - loss: 0.9233 - accuracy: 0.6463\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7836 - accuracy: 0.7008WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 161s 5s/step - loss: 0.7836 - accuracy: 0.7008\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.6640 - accuracy: 0.7505WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 168s 5s/step - loss: 0.6640 - accuracy: 0.7505\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4931 - accuracy: 0.8293WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 183s 5s/step - loss: 0.4931 - accuracy: 0.8293\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8659WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 183s 5s/step - loss: 0.3857 - accuracy: 0.8659\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.8856WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 189s 6s/step - loss: 0.3271 - accuracy: 0.8856\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9034WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 180s 5s/step - loss: 0.2587 - accuracy: 0.9034\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9156WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 185s 5s/step - loss: 0.2290 - accuracy: 0.9156\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9325WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 181s 5s/step - loss: 0.1780 - accuracy: 0.9325\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.9475WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "34/34 [==============================] - 178s 5s/step - loss: 0.1537 - accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
    "cb = keras.callbacks.ModelCheckpoint('cnn_model.hdf5')\n",
    "es_cb = keras.callbacks.EarlyStopping(patience=2,restore_best_weights=True)\n",
    "h = model.fit(xtrain_scaled,ytrain,epochs=15,\n",
    "              callbacks=[cb,es_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 10s 999ms/step - loss: 1.2976 - accuracy: 0.6142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2975951433181763, 0.6142321825027466]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result = model.evaluate(xtest_scaled, ytest)\n",
    "evaluate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\physiognomy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 120, 120, 120)     1200      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 60, 60, 120)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 240)       259440    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 60, 240)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 240)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 216000)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               25920120  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                7260      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 427       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26188447 (99.90 MB)\n",
      "Trainable params: 26188447 (99.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m =  tf.keras.models.load_model('cnn_model.hdf5')\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "34/34 [==============================] - 32s 923ms/step - loss: 0.2663 - accuracy: 0.9296\n",
      "[0.26632580161094666, 0.9296435117721558]\n",
      "9/9 [==============================] - 9s 966ms/step - loss: 0.4068 - accuracy: 0.8914\n",
      "[0.4067699909210205, 0.8913857936859131]\n"
     ]
    }
   ],
   "source": [
    "print(m.evaluate(xtrain_scaled, ytrain))\n",
    "print(m.evaluate(xtest_scaled, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "[[1.1348886e-08 1.8937330e-01 1.0388881e-06 1.8899280e-04 2.1644415e-02\n",
      "  7.8845108e-01 3.4109954e-04]]\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Predicted class: 너는 치와와 닮음\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('seonggi.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (120,120), interpolation=cv2.INTER_LINEAR)\n",
    "img_scaled = (img / 255).reshape(120,120,1)\n",
    "img_scaled = np.expand_dims(img_scaled, axis=0)\n",
    "\n",
    "print(m.predict(img_scaled))\n",
    "predicted_class = np.argmax(m.predict(img_scaled), axis=1)\n",
    "predicted_class\n",
    "\n",
    "dic = {1: '너는 비글 닮음', 2: '너는 포메 닮음', 3: '너는 퍼그 닮음', 4: '너는 허스키 닮음',5:'너는 치와와 닮음',6:'너느 골든 리트리버 닮음'}\n",
    "print(\"Predicted class:\", dic[predicted_class[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
